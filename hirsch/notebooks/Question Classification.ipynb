{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification\n",
    "\n",
    "In document classification, we aim to categorize each document with a label from a predetermined set. \n",
    "\n",
    "In this example, we will demonstrate how to classify potential user questions **textblob**.\n",
    "\n",
    "Please see http://cogcomp.cs.illinois.edu/Data/QA/QC/ for more information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import qc, reuters\n",
    "import nltk\n",
    "import pprint\n",
    "import warnings\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset we will use contains 6,000 questions that are labelled into one of 50 categories. There are 6 high level categories:\n",
    "* Abbreviation: ABB\n",
    "* Entity: ENT\n",
    "* Description: DESC\n",
    "* Human: HUM\n",
    "* Location: LOC\n",
    "* Numeric: NUM\n",
    "\n",
    "These categories are further dividided into subcategories, but for the purposes of this demonstration, we will only concern oursevles with the highest levels.\n",
    "\n",
    "Since the goal of this tutorial is classification, the first thing we will do is split our data into training and test sets. Even though the data is already split as provided, we will combine all questions and split it ourselves into an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = qc.tuples(\"train.txt\") + qc.tuples(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'DESC:def', u'What is an annotated bibliography ?')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = [(q[1], q[0].split(\":\")[0]) for q in qc.tuples(\"train.txt\")]\n",
    "test = [(q[1], q[0].split(\":\")[0]) for q in qc.tuples(\"test.txt\")]\n",
    "full = test + train\n",
    "len(full)\n",
    "\n",
    "train = full[:4760]\n",
    "test = full[4760:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'How far is it from Denver to Aspen ?', u'NUM')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = [q[0] for q in train]\n",
    "c_train = [q[1] for q in train]\n",
    "q_test = [q[0] for q in test]\n",
    "c_test = [q[1] for q in test]\n",
    "q_full = [q[0] for q in test]\n",
    "c_full = [q[1] for q in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = pipeline.fit(q_train, c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'HUM'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Who is Michael?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77432885906040272"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(q_test)\n",
    "np.mean(predicted == c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "http://text-processing.com/demo/\n",
    "\n",
    "http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "http://wit.ai\n",
    "\n",
    "ibm tone analyzer\n",
    "\n",
    "https://api.ai/\n",
    "\n",
    "https://algorithmia.com/tags/text%20analysis\n",
    "\n",
    "https://demos.explosion.ai/sense2vec/?word=natural%20language%20processing&sense=auto\n",
    "\n",
    "https://spacy.io/\n",
    "\n",
    "https://explosion.ai/\n",
    "\n",
    "http://blog.aylien.com/naive-bayes-for-dummies-a-simple-explanation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
